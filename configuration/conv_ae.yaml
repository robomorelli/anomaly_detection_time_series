# Dataset
dataset:
  name: 'all_2016-2018_clean_std_4s.pkl'
  sequence_length: 16
  #columns: ['RW1_motcurr', 'RW2_motcurr', 'RW3_motcurr', 'RW4_motcurr',
  #          'RW1_cmd_volt', 'RW2_cmd_volt','RW3_cmd_volt', 'RW4_cmd_volt',
  #          'RW1_therm','RW2_therm', 'RW3_therm', 'RW4_therm',
  #          'RW1_speed', 'RW2_speed', 'RW3_speed', 'RW4_speed']
  columns: ['RW3_motcurr','RW3_cmd_volt', 'RW3_therm','RW3_speed']
  train_val_split: 0.70
  shuffle: 0
  columns_subset: 0
  dataset_subset: 1000000   #subset of rows to select from the original atabases
  batch_size: 500
  sampling_rate: '4s'
  scaled: 1   # standardize the data values
  target: null # predict or reconstruct one or more specific features
  forecast: 0 # forecast one or more specific values
  forecast_all: 0 # forecast all the columns
  predict: 0 # the columns to forecast is dropped from the original database (in the forecast version the column/s is/are not dropped)

opt:
  epochs: 200
  lr: 0.001
  lr_patience: 5 # learning rate patience
  es_patience: 10 #early stopping patience

model:
  architecture: 'conv_ae'
  increasing: 0 # from filter num double the number of filter layewr by layer ( to use in combination with flattened to limit the latent space dim to "latent space")
  flattened: 1 # flatten the latent space from convolutional filter (generally true for conv 2d)
  latent_dim: 100 # effectiv only with flattened = 1
  stride: 1
  kernel_size: 3
  filter_num: 64
  n_layers: 2 #number of layer added to the initial one: 2 layer mean 3 layers
  activation: 'Relu' # (Relu or Elu implemented)
  bn: 1 #use batch nornmalization
